{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPc1zoRdCbHQ+n8dLOgwXpL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HenryZumaeta/MISCELANEAS/blob/Zeta/PYTHON/Asociacion_Numerico_Categorico_Correlacion_Incertidumbre.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replicando el correlograma de la librería sweetviz"
      ],
      "metadata": {
        "id": "FGb1sorXq5p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import Normalize\n",
        "from matplotlib.cm import ScalarMappable\n",
        "from scipy.stats import chi2_contingency, entropy\n",
        "\n",
        "def calculate_correlation_matrix(df):\n",
        "    # Calcula la matriz de correlación de Pearson para variables numéricas\n",
        "    corr_matrix = df.corr(method='pearson')\n",
        "    return corr_matrix\n",
        "\n",
        "def calculate_uncertainty_coefficient(df, cat_cols):\n",
        "    # Calcula el coeficiente de incertidumbre para variables categóricas\n",
        "    def uncertainty_coefficient(x, y):\n",
        "        contingency_table = pd.crosstab(x, y)\n",
        "        chi2 = chi2_contingency(contingency_table)[0]\n",
        "        n = contingency_table.sum().sum()\n",
        "        entropy_x = entropy(x.value_counts(normalize=True))\n",
        "        entropy_y = entropy(y.value_counts(normalize=True))\n",
        "        return chi2 / n / (min(entropy_x, entropy_y))\n",
        "\n",
        "    n = len(cat_cols)\n",
        "    uc_matrix = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if i == j:\n",
        "                uc_matrix[i, j] = 1\n",
        "            else:\n",
        "                uc_matrix[i, j] = uncertainty_coefficient(df[cat_cols[i]], df[cat_cols[j]])\n",
        "    return pd.DataFrame(uc_matrix, index=cat_cols, columns=cat_cols)\n",
        "\n",
        "def plot_associations(df):\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    cat_cols = df.select_dtypes(include=[object, 'category']).columns\n",
        "\n",
        "    if numeric_cols.empty:\n",
        "        raise ValueError(\"No numeric columns found in the dataset.\")\n",
        "    if cat_cols.empty:\n",
        "        raise ValueError(\"No categorical columns found in the dataset.\")\n",
        "\n",
        "    corr_matrix = calculate_correlation_matrix(df[numeric_cols])\n",
        "    uc_matrix = calculate_uncertainty_coefficient(df, cat_cols)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    norm = Normalize(vmin=-1, vmax=1)\n",
        "    cmap = plt.cm.bwr\n",
        "\n",
        "    n = len(df.columns)\n",
        "    ax.set_xlim(0, n)\n",
        "    ax.set_ylim(0, n)\n",
        "\n",
        "    found_values = False\n",
        "\n",
        "    for i, col1 in enumerate(df.columns):\n",
        "        for j, col2 in enumerate(df.columns):\n",
        "            if col1 in numeric_cols and col2 in numeric_cols:\n",
        "                corr_value = corr_matrix.loc[col1, col2]\n",
        "                if not np.isnan(corr_value):\n",
        "                    size = abs(corr_value) * 1000\n",
        "                    color = cmap(norm(corr_value))\n",
        "                    shape = 'o'\n",
        "                    ax.scatter(j, n - i - 1, s=size, c=[color], marker=shape)\n",
        "                    found_values = True\n",
        "            elif col1 in cat_cols and col2 in cat_cols:\n",
        "                uc_value = uc_matrix.loc[col1, col2]\n",
        "                if not np.isnan(uc_value):\n",
        "                    size = uc_value * 1000\n",
        "                    color = cmap(norm(uc_value))\n",
        "                    shape = 's'\n",
        "                    ax.scatter(j, n - i - 1, s=size, c=[color], marker=shape)\n",
        "                    found_values = True\n",
        "\n",
        "    if not found_values:\n",
        "        print(\"No valid associations found to plot.\")\n",
        "        return\n",
        "\n",
        "    ax.set_xticks(range(n))\n",
        "    ax.set_xticklabels(df.columns, rotation=90)\n",
        "    ax.set_yticks(range(n))\n",
        "    ax.set_yticklabels(df.columns[::-1])\n",
        "\n",
        "    sm = ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])\n",
        "    cbar = fig.colorbar(sm, ax=ax)\n",
        "    cbar.set_label('Correlation')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Uso: simulacion de data 'raleo_base_clus_clusteres.csv'\n",
        "data = {\n",
        "    'data_Tcamp': [1, 2, 3, 4, 5],\n",
        "    'data_sumCOD': [2, 3, 4, 5, 6],\n",
        "    'data_sumHRSLB': [3, 4, 5, 6, 7],\n",
        "    'data_GRAD02': ['PRIMARIA COMPLETA', 'SECUNDARIA COMPLETA', 'PRIMARIA INCOMPLETA', 'SECUNDARIA COMPLETA', 'SECUNDARIA INCOMPLETA'],\n",
        "    'data_sum.variedad_total_AC': [5, 6, 7, 8, 9],\n",
        "    'data_sum.variedad_total_CC': [6, 7, 8, 9, 10],\n",
        "    'data_sum.variedad_total_CP': [7, 8, 9, 10, 11],\n",
        "    'data_sum.variedad_total_LS': [8, 9, 10, 11, 12]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "plot_associations(df)"
      ],
      "metadata": {
        "id": "XUxwUpIfm3Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "def calculate_correlation(df):\n",
        "    # Calcula la correlación de Pearson solo para columnas numéricas\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "    correlation_matrix = numeric_df.corr(method='pearson')\n",
        "    return correlation_matrix\n",
        "\n",
        "def cramers_v(x, y):\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    return np.sqrt(chi2 / (n * (min(confusion_matrix.shape) - 1)))\n",
        "\n",
        "def calculate_association(df):\n",
        "    cols = df.columns\n",
        "    association_matrix = pd.DataFrame(index=cols, columns=cols)\n",
        "\n",
        "    for i in range(len(cols)):\n",
        "        for j in range(i, len(cols)):\n",
        "            col1 = df[cols[i]]\n",
        "            col2 = df[cols[j]]\n",
        "\n",
        "            if col1.dtype == 'object' or col2.dtype == 'object':\n",
        "                association_matrix.iat[i, j] = cramers_v(col1, col2)\n",
        "                association_matrix.iat[j, i] = association_matrix.iat[i, j]\n",
        "            else:\n",
        "                association_matrix.iat[i, j] = np.nan\n",
        "                association_matrix.iat[j, i] = np.nan\n",
        "\n",
        "    return association_matrix\n",
        "\n",
        "def plot_association_correlation(df, title, note):\n",
        "    corr = calculate_correlation(df)\n",
        "    assoc = calculate_association(df)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.title(title)\n",
        "\n",
        "    sns.heatmap(corr, annot=False, cmap=\"coolwarm\", center=0, cbar_kws={'label': 'Correlación (Pearson)'}, square=True, linewidths=.5)\n",
        "\n",
        "    for i in range(len(corr.columns)):\n",
        "        for j in range(len(corr.columns)):\n",
        "            if i != j:\n",
        "                size = abs(assoc.iloc[i, j])\n",
        "                if not np.isnan(size):\n",
        "                    plt.gca().add_patch(mpatches.Rectangle((j, i), 1, 1, fill=False, edgecolor='blue', lw=size * 5, linestyle='-', alpha=0.3))\n",
        "\n",
        "    for i in range(len(corr.columns)):\n",
        "        for j in range(len(corr.columns)):\n",
        "            if i != j:\n",
        "                size = abs(corr.iloc[i, j])\n",
        "                if not np.isnan(size):\n",
        "                    plt.gca().add_patch(plt.Circle((j + 0.5, i + 0.5), size / 2, color='blue', alpha=0.3))\n",
        "\n",
        "    plt.xticks(np.arange(len(df.columns)) + 0.5, df.columns, rotation=90)\n",
        "    plt.yticks(np.arange(len(df.columns)) + 0.5, df.columns, rotation=0)\n",
        "    plt.gca().set_xticks(np.arange(len(df.columns)) + 0.5, minor=True)\n",
        "    plt.gca().set_yticks(np.arange(len(df.columns)) + 0.5, minor=True)\n",
        "    plt.gca().grid(False, which='minor', color='black', linestyle='-', linewidth=2)\n",
        "\n",
        "    plt.figtext(0.5, -0.1, note, wrap=True, horizontalalignment='center', fontsize=10, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Título y nota\n",
        "title = \"Asociaciones\"\n",
        "note = (\"Solo incluyendo el conjunto de datos analizado\"\n",
        "        \"■ Los cuadrados son asociaciones categóricas (coeficiente de incertidumbre y razón de correlación) de 0 a 1. \"\n",
        "        \"El coeficiente de incertidumbre es asimétrico, (es decir, los valores de la ETIQUETA DE FILA indican cuánto INFORMAN a cada ETIQUETA en la PARTE SUPERIOR). \"\n",
        "        \"• Los círculos son las correlaciones numéricas simétricas (Pearson) de -1 a 1. La diagonal trivial se deja intencionalmente en blanco para mayor claridad.\")\n",
        "\n",
        "\n",
        "# Gráfica\n",
        "plot_association_correlation(df, title, note)\n"
      ],
      "metadata": {
        "id": "Hl5l3zc2m-Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import matplotlib.patches as patches\n",
        "from textwrap import wrap\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Definiciones globales y configuraciones\n",
        "CORRELATION_ERROR = 83572398457329.0\n",
        "CORRELATION_IDENTICAL = 1357239845732.0\n",
        "\n",
        "def wrap_custom(source_text, separator_chars, width=70, keep_separators=True):\n",
        "    current_length = 0\n",
        "    latest_separator = -1\n",
        "    current_chunk_start = 0\n",
        "    output = \"\"\n",
        "    char_index = 0\n",
        "    while char_index < len(source_text):\n",
        "        if source_text[char_index] in separator_chars:\n",
        "            latest_separator = char_index\n",
        "        output += source_text[char_index]\n",
        "        current_length += 1\n",
        "        if current_length == width:\n",
        "            if latest_separator >= current_chunk_start:\n",
        "                cutting_length = char_index - latest_separator\n",
        "                if not keep_separators:\n",
        "                    cutting_length += 1\n",
        "                if cutting_length:\n",
        "                    output = output[:-cutting_length]\n",
        "                output += \"\\n\"\n",
        "                current_chunk_start = latest_separator + 1\n",
        "                char_index = current_chunk_start\n",
        "            else:\n",
        "                output += \"\\n\"\n",
        "                current_chunk_start = char_index + 1\n",
        "                latest_separator = current_chunk_start - 1\n",
        "                char_index += 1\n",
        "            current_length = 0\n",
        "        else:\n",
        "            char_index += 1\n",
        "    return output\n",
        "\n",
        "def cramers_v(x, y):\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    if confusion_matrix.shape[0] <= 1 or confusion_matrix.shape[1] <= 1:\n",
        "        return np.nan\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    return np.sqrt(chi2 / (n * (min(confusion_matrix.shape) - 1)))\n",
        "\n",
        "def make_zero_square_dataframe(features):\n",
        "    new_dataframe = pd.DataFrame()\n",
        "    for feature in features:\n",
        "        new_dataframe[feature] = pd.Series(dtype=float)\n",
        "    return new_dataframe.reindex(list(range(0, len(features)))).reset_index(drop=True).fillna(0.0)\n",
        "\n",
        "def calculate_association(df):\n",
        "    cols = df.columns\n",
        "    association_matrix = pd.DataFrame(index=cols, columns=cols)\n",
        "\n",
        "    for i in range(len(cols)):\n",
        "        for j in range(i, len(cols)):\n",
        "            col1 = df[cols[i]]\n",
        "            col2 = df[cols[j]]\n",
        "\n",
        "            if col1.dtype == 'object' or col2.dtype == 'object':\n",
        "                association_matrix.iat[i, j] = cramers_v(col1, col2)\n",
        "                association_matrix.iat[j, i] = association_matrix.iat[i, j]\n",
        "            else:\n",
        "                association_matrix.iat[i, j] = np.nan\n",
        "                association_matrix.iat[j, i] = np.nan\n",
        "\n",
        "    return association_matrix\n",
        "\n",
        "def calculate_correlation(df):\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "    correlation_matrix = numeric_df.corr(method='pearson')\n",
        "    return correlation_matrix\n",
        "\n",
        "def corrplot(correlation_dataframe, dataframe_report, size_scale=100, marker='s'):\n",
        "    corr = pd.melt(correlation_dataframe.reset_index(), id_vars='index')\n",
        "    corr.columns = ['x', 'y', 'value']\n",
        "\n",
        "    def heatmap(y, x, figure_size, **kwargs):\n",
        "        color = kwargs.get('color', [1]*len(x))\n",
        "        palette = [(0.85, (0.85/128)*i, (0.85/128)*i) for i in range(0,128)] + [(0.85 - 0.85*(i-128.0)/128.0, 0.85 - 0.85*(i-128.0)/128.0, 0.85) for i in range(128,256)]\n",
        "        color_min, color_max = kwargs.get('color_range', (min(color), max(color)))\n",
        "\n",
        "        def value_to_color(val):\n",
        "            if color_min == color_max:\n",
        "                return palette[-1]\n",
        "            if val == CORRELATION_IDENTICAL or val == CORRELATION_ERROR:\n",
        "                return palette[-1]\n",
        "            val_position = float((val - color_min)) / (color_max - color_min)\n",
        "            val_position = min(max(val_position, 0), 1)\n",
        "            val_position = math.pow(val_position, 0.925)\n",
        "            ind = int(val_position * 255)\n",
        "            return palette[ind]\n",
        "\n",
        "        size = kwargs.get('size', [1]*len(x))\n",
        "        size_min, size_max = kwargs.get('size_range', (min(size), max(size)))\n",
        "        size_scale = kwargs.get('size_scale', 500) / len(x)\n",
        "\n",
        "        def value_to_size(val):\n",
        "            if val == 0 or val == abs(CORRELATION_IDENTICAL) or val == abs(CORRELATION_ERROR):\n",
        "                return 0.0\n",
        "            if size_min == size_max:\n",
        "                return 1 * size_scale\n",
        "            val_position = (val - size_min) * 0.999 / (size_max - size_min) + 0.001\n",
        "            val_position = min(max(val_position, 0), 1)\n",
        "            val_position = math.pow(val_position, 0.5)\n",
        "            return val_position\n",
        "\n",
        "        def do_wrapping(label, length):\n",
        "            return wrap_custom(label, [\"_\", \"-\"], length)\n",
        "\n",
        "        wrap_x = 12\n",
        "        wrap_y = 13\n",
        "        x_names = [t for t in kwargs.get('x_order', sorted(set([v for v in x])))]\n",
        "        x_names = [do_wrapping(label, wrap_x) for label in x_names]\n",
        "        x_to_num = {p[1]:p[0] for p in enumerate(x_names)}\n",
        "\n",
        "        y_names = [t for t in kwargs.get('y_order', sorted(set([v for v in y])))]\n",
        "        y_names = [do_wrapping(label, wrap_y) for label in y_names]\n",
        "        y_to_num = {p[1]:p[0] for p in enumerate(y_names)}\n",
        "\n",
        "        figure, axs = plt.subplots(1, 1, figsize=figure_size)\n",
        "\n",
        "        marker = kwargs.get('marker', 's')\n",
        "\n",
        "        kwargs_pass_on = {k:v for k,v in kwargs.items() if k not in ['color', 'palette', 'color_range', 'size', 'size_range', 'size_scale', 'marker', 'x_order', 'y_order']}\n",
        "\n",
        "        axs.tick_params(labelbottom='on', labeltop='on')\n",
        "        axs.set_xticks([v for k,v in x_to_num.items()])\n",
        "        axs.set_xticklabels([k for k in x_to_num], rotation=90, horizontalalignment='center', linespacing=0.8)\n",
        "        axs.set_yticks([v for k,v in y_to_num.items()])\n",
        "        axs.set_yticklabels([k for k in y_to_num], linespacing=0.85)\n",
        "\n",
        "        axs.grid(False, 'major')\n",
        "        axs.grid(True, 'minor')\n",
        "        axs.set_xticks([t + 0.5 for t in axs.get_xticks()], minor=True)\n",
        "        axs.set_yticks([t + 0.5 for t in axs.get_yticks()], minor=True)\n",
        "\n",
        "        axs.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5])\n",
        "        axs.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])\n",
        "        axs.set_facecolor('#F1F1F1')\n",
        "\n",
        "        delta_in_pix = axs.transData.transform((1, 1)) - axs.transData.transform((0, 0))\n",
        "\n",
        "        index = 0\n",
        "        for cur_x, cur_y in zip(x,y):\n",
        "            wrapped_x_name = do_wrapping(cur_x, wrap_x)\n",
        "            wrapped_y_name = do_wrapping(cur_y, wrap_y)\n",
        "            before_coordinate = np.array(axs.transData.transform((x_to_num[wrapped_x_name]-0.5, y_to_num[wrapped_y_name] -0.5)))\n",
        "            after_coordinate = np.array(axs.transData.transform((x_to_num[wrapped_x_name]+0.5, y_to_num[wrapped_y_name] +0.5)))\n",
        "            before_pixels = np.round(before_coordinate, 0)\n",
        "            after_pixels = np.round(after_coordinate, 0)\n",
        "            desired_fraction = value_to_size(size[index])\n",
        "            if desired_fraction == 0.0:\n",
        "                index += 1\n",
        "                continue\n",
        "            use_rectangle = True if dataframe_report[cur_x][\"type\"] != \"NUM\" or dataframe_report[cur_y][\"type\"] != \"NUM\" else False\n",
        "            delta_in_pix = after_pixels - before_pixels\n",
        "            gap = np.round((1.0 - desired_fraction) * delta_in_pix / 2, 0)\n",
        "            start = before_pixels + gap[0]\n",
        "            ending = after_pixels - gap[0]\n",
        "            start[0] += 1\n",
        "            ending[1] -= 1\n",
        "            start_doc = axs.transData.inverted().transform(start)\n",
        "            ending_doc = axs.transData.inverted().transform(ending)\n",
        "            cur_size = ending_doc - start_doc\n",
        "            if use_rectangle:\n",
        "                                    cur_rect = patches.Rectangle((start_doc[0], start_doc[1]), cur_size[0], cur_size[1], facecolor=value_to_color(color[index]), antialiased=True)\n",
        "            else:\n",
        "                cur_rect = patches.Circle((start_doc[0] + cur_size[0] / 2, start_doc[1] + cur_size[1] / 2), cur_size[0] / 2, facecolor=value_to_color(color[index]), antialiased=True)\n",
        "            cur_rect.set_antialiased(True)\n",
        "            axs.add_patch(cur_rect)\n",
        "            index += 1\n",
        "\n",
        "        if color_min < color_max:\n",
        "            ax = plt.subplot2grid((1, 15), (0, 14))\n",
        "            col_x = [0] * len(palette)\n",
        "            bar_y = np.linspace(color_min, color_max, len(palette))\n",
        "            ax.set_ylim(-1, 1)\n",
        "            bar_height = bar_y[1] - bar_y[0]\n",
        "            ax.barh(\n",
        "                y=bar_y,\n",
        "                width=[5] * len(palette),\n",
        "                left=col_x,\n",
        "                height=bar_height,\n",
        "                color=palette,\n",
        "                linewidth=0)\n",
        "            ax.set_xlim(1, 2)\n",
        "            ax.grid(False)\n",
        "            ax.set_facecolor('white')\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks(np.linspace(min(bar_y), max(bar_y), 3))\n",
        "            ax.yaxis.tick_right()\n",
        "        return figure\n",
        "\n",
        "    return heatmap(\n",
        "        corr['y'], corr['x'],\n",
        "        figure_size=(20, 15),\n",
        "        color=corr['value'], color_range=[-1, 1],\n",
        "        size=corr['value'].abs(), size_range=[0, 1],\n",
        "        marker=marker,\n",
        "        x_order=correlation_dataframe.columns,\n",
        "        y_order=correlation_dataframe.columns[::-1],\n",
        "        size_scale=size_scale,\n",
        "        dataframe_report=dataframe_report\n",
        "    )\n",
        "\n",
        "def create_dataframe_report(df):\n",
        "    dataframe_report = {}\n",
        "    for col in df.columns:\n",
        "        col_type = 'NUM' if pd.api.types.is_numeric_dtype(df[col]) else 'CAT'\n",
        "        dataframe_report[col] = {\"type\": col_type}\n",
        "    return dataframe_report\n",
        "\n",
        "def plot_association_correlation(df, title, note):\n",
        "    # Crear el reporte del dataframe\n",
        "    dataframe_report = create_dataframe_report(df)\n",
        "\n",
        "    # Calcular correlaciones y asociaciones\n",
        "    corr = calculate_correlation(df)\n",
        "    assoc = calculate_association(df)\n",
        "\n",
        "    # Crear un dataframe combinado para la visualización\n",
        "    combined = corr.copy()\n",
        "    for col in assoc.columns:\n",
        "        if col not in combined.columns:\n",
        "            combined[col] = np.nan\n",
        "    for index, row in assoc.iterrows():\n",
        "        for col in assoc.columns:\n",
        "            combined.at[index, col] = assoc.at[index, col]\n",
        "\n",
        "    # Generar la gráfica\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    plt.title(title)\n",
        "\n",
        "    corrplot(combined, dataframe_report, size_scale=100, marker='s')\n",
        "\n",
        "    # Añadir la nota\n",
        "    plt.figtext(0.5, -0.1, note, wrap=True, horizontalalignment='center', fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Nota y título\n",
        "title = \"Asociaciones\"\n",
        "note = (\"Solo incluyendo el conjunto de datos analizado\"\n",
        "        \"■ Los cuadrados son asociaciones categóricas (coeficiente de incertidumbre y razón de correlación) de 0 a 1. \"\n",
        "        \"El coeficiente de incertidumbre es asimétrico, (es decir, los valores de la ETIQUETA DE FILA indican cuánto INFORMAN a cada ETIQUETA en la PARTE SUPERIOR). \"\n",
        "        \"• Los círculos son las correlaciones numéricas simétricas (Pearson) de -1 a 1. La diagonal trivial se deja intencionalmente en blanco para mayor claridad.\")\n",
        "\n",
        "# Gráfica\n",
        "plot_association_correlation(df, title, note)"
      ],
      "metadata": {
        "id": "3DwYJrK_nDb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cUYLhN1mQxif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Mar  11 09:19:35 2019\n",
        "\n",
        "@author: alejandrokoury\n",
        "\"\"\"\n",
        "\n",
        "SEED = 1\n",
        "TARGET_VARIABLE = 'cnt'\n",
        "SPLITS = 4\n",
        "ESTIMATORS = 50\n",
        "METRIC = 'r2'\n",
        "TIMESERIES = True\n",
        "\n",
        "if METRIC == 'r2':\n",
        "    from sklearn.metrics import r2_score as metric_scorer\n",
        "else:\n",
        "    from sklearn.metrics import accuracy_score as metric_scorer\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tempfile import mkdtemp\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.stats import chi2_contingency\n",
        "from scipy.stats.mstats import winsorize\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, TimeSeriesSplit, StratifiedKFold\n",
        "\n",
        "\n",
        "def missing_data(df):\n",
        "    total = df.isnull().sum().sort_values(ascending=False)\n",
        "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
        "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "\n",
        "def convert_to_category(df, cols):\n",
        "    for i in cols:\n",
        "        df[i] = df[i].astype('category')\n",
        "    return df\n",
        "\n",
        "def drop_columns(df, cols):\n",
        "    return df.drop(df[cols], axis=1)\n",
        "\n",
        "def types(df, types, exclude = None):\n",
        "    types = df.select_dtypes(include=types)\n",
        "    excluded = [TARGET_VARIABLE]\n",
        "    if exclude:\n",
        "        for i in exclude:\n",
        "            excluded.append(i)\n",
        "    cols = [col for col in types.columns if col not in excluded]\n",
        "    return df[cols]\n",
        "\n",
        "def numericals(df, exclude = None):\n",
        "    return types(df, [np.number], exclude)\n",
        "\n",
        "def categoricals(df, exclude = None):\n",
        "    return types(df, ['category', object], exclude)\n",
        "\n",
        "def numerical_correlated(df, threshold=0.9):\n",
        "    corr_matrix = np.absolute(df.select_dtypes(include=[np.number]).corr(method='spearman')).abs()\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "    return [column for column in upper.columns if any(abs(upper[column]) > threshold)], corr_matrix\n",
        "\n",
        "def cramers_v(x, y):\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2/n\n",
        "    r, k = confusion_matrix.shape\n",
        "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
        "    rcorr = r-((r-1)**2)/(n-1)\n",
        "    kcorr = k-((k-1)**2)/(n-1)\n",
        "    return np.sqrt(phi2corr/min((kcorr-1), (rcorr-1)))\n",
        "\n",
        "def categorical_correlated(df, threshold=0.9):\n",
        "    columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    corr = pd.DataFrame(index=columns, columns=columns, dtype=float)  # Asegurando que el DataFrame es de tipo float\n",
        "    for i in range(0, len(columns)):\n",
        "        for j in range(i+1, len(columns)):  # Asegura que no se compara la columna consigo misma\n",
        "            if df[columns[i]].dtype.name == 'category' and df[columns[j]].dtype.name == 'category':\n",
        "                # Asegurar que ambos campos son categóricos y están codificados como códigos de categoría\n",
        "                val1 = df[columns[i]].cat.codes if df[columns[i]].dtype.name == 'category' else df[columns[i]]\n",
        "                val2 = df[columns[j]].cat.codes if df[columns[j]].dtype.name == 'category' else df[columns[j]]\n",
        "                cell = cramers_v(val1, val2)\n",
        "                corr.at[columns[i], columns[j]] = cell\n",
        "                corr.at[columns[j], columns[i]] = cell\n",
        "    corr.fillna(value=np.nan, inplace=True)\n",
        "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "    return [column for column in upper.columns if any(upper[column] > threshold)], corr\n",
        "\n",
        "\n",
        "\n",
        "def correlated(df, threshold = 0.9):\n",
        "    categoric = categorical_correlated(df, threshold)\n",
        "    numeric = numerical_correlated(df, threshold)\n",
        "\n",
        "    plt.figure(figsize=(12,10))\n",
        "    sns.heatmap(categoric[1],cbar=True,fmt =' .2f', annot=True, cmap='viridis').set_title('Categorical Correlation', fontsize=30)\n",
        "\n",
        "    plt.figure(figsize=(12,10))\n",
        "    sns.heatmap(numeric[1],cbar=True,fmt =' .2f', annot=True, cmap='viridis').set_title('Numerical Correlation', fontsize=30)\n",
        "\n",
        "    correlated_cols = categoric[0] + numeric[0]\n",
        "\n",
        "    if(len(correlated_cols) > 0):\n",
        "        print('The following columns are correlated with a threshold of ' + str(threshold) + ': ' + str(correlated_cols))\n",
        "    else:\n",
        "        print('No correlated columns for the  ' + str(threshold) + ' threshold')\n",
        "\n",
        "    return correlated_cols, categoric[1], numeric[1]\n",
        "\n",
        "def winsorize_data(df, train_df, cols):\n",
        "    for col in cols:\n",
        "        train_df[col] = winsorize(train_df[col], limits = [0.01, 0.01])\n",
        "        df[df[col] > max(train_df[col])][col] = max(train_df[col])\n",
        "        df[df[col] < min(train_df[col])][col] = min(train_df[col])\n",
        "    return df\n",
        "\n",
        "def lof(df, training_df):\n",
        "    lof = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n",
        "    y_pred = lof.fit_predict(training_df)\n",
        "    outliers = np.where(y_pred == -1)\n",
        "    print('Removing ' + str(len(outliers[0])) + ' records')\n",
        "    return df.drop(outliers[0])\n",
        "\n",
        "def one_hot_encode(df, cols):\n",
        "    for i in cols:\n",
        "        dummies = pd.get_dummies(df[i], prefix=i, drop_first = False)\n",
        "        df = pd.concat([df, dummies], axis = 1)\n",
        "        df = df.drop(i, axis = 1)\n",
        "\n",
        "    return df\n",
        "\n",
        "def under_represented_features(df, threshold = 0.99, holdout_df = None):\n",
        "    under_rep = []\n",
        "    for column in df:\n",
        "        counts = df[column].value_counts()\n",
        "        majority_freq = counts.iloc[0]\n",
        "        if (majority_freq / len(df)) > threshold:\n",
        "            under_rep.append(column)\n",
        "\n",
        "    if not under_rep:\n",
        "        print('No underrepresented features')\n",
        "    else:\n",
        "        if TARGET_VARIABLE in under_rep:\n",
        "            print('The target variable is underrepresented, consider rebalancing')\n",
        "            under_represented.remove(TARGET_VARIABLE)\n",
        "        print(str(under_rep) + ' underrepresented, removing')\n",
        "\n",
        "    df = drop_columns(df, under_rep)\n",
        "\n",
        "    if holdout_df is not None:\n",
        "        return df, drop_columns(holdout_df, under_rep)\n",
        "\n",
        "    return df\n",
        "\n",
        "def feature_importance(df, model):\n",
        "    acc, scores, model = cv_evaluate(df, model = model)\n",
        "    importances = model.feature_importances_\n",
        "    std = np.std([tree.feature_importances_ for tree in model.estimators_],axis=0)\n",
        "    indices = np.argsort(importances)\n",
        "\n",
        "    X = df.loc[:, df.columns != TARGET_VARIABLE]\n",
        "    print(\"Feature ranking:\")\n",
        "    plt.figure(figsize=(16, 14))\n",
        "    plt.title(\"Feature importances\")\n",
        "    plt.barh(range(X.shape[1]), importances[indices],color=\"r\", xerr=std[indices], align=\"center\")\n",
        "    plt.yticks(range(X.shape[1]), [list(df.loc[:, df.columns != TARGET_VARIABLE])[i] for i in indices])\n",
        "    plt.ylim([-1, X.shape[1]])\n",
        "    plt.show()\n",
        "\n",
        "def plot_pca_components(df, variance = 0.9):\n",
        "    X = df.loc[:, df.columns != TARGET_VARIABLE]\n",
        "    y = df.loc[:, TARGET_VARIABLE]\n",
        "    pca = PCA().fit(X)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Variance (%)')\n",
        "    plt.show()\n",
        "\n",
        "def cv_evaluate(df, model, splits = SPLITS, transformers = None, grid = None):\n",
        "    X = df.loc[:, df.columns != TARGET_VARIABLE]\n",
        "    y = df.loc[:, TARGET_VARIABLE]\n",
        "    if TIMESERIES:\n",
        "        folds = TimeSeriesSplit(n_splits = splits)\n",
        "    else:\n",
        "        folds = StratifiedKFold(n_splits = splits, shuffle = True, random_state=SEED)\n",
        "\n",
        "    train_size = int(len(df) * 0.85)\n",
        "    X_train, X_validate, y_train, y_validate = X[0:train_size], X[train_size:len(df)], y[0:train_size], y[train_size:len(df)]\n",
        "\n",
        "    if transformers:\n",
        "        cachedir = mkdtemp()\n",
        "        model = make_pipeline(model, memory = cachedir)\n",
        "        for ind,i in enumerate(transformers):\n",
        "            model.steps.insert(ind,[str(ind+1),i])\n",
        "\n",
        "    if grid:\n",
        "        model = RandomizedSearchCV(model, grid, scoring = METRIC, cv = folds, n_iter = 20, refit=True, return_train_score = False, error_score=0.0, random_state = SEED)\n",
        "        model.fit(X_train, y_train)\n",
        "        scores = model.cv_results_['mean_test_score']\n",
        "    else:\n",
        "        scores = cross_val_score(model, X_train, y_train, scoring = METRIC, cv = folds)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    pred = model.predict(X_validate)\n",
        "    final_score = metric_scorer(y_validate, pred)\n",
        "\n",
        "    return final_score, scores, model\n",
        "\n",
        "def feature_engineering_pipeline(df, models, transformers, splits = SPLITS):\n",
        "    all_scores  = pd.DataFrame(columns = ['Model', 'Function', 'CV Score', 'Holdout Score', 'Difference', 'Outcome'])\n",
        "\n",
        "    for model in models:\n",
        "        top_cv_score, cv_scores, cv_model = cv_evaluate(df, model = model['model'], splits = splits)\n",
        "        model['score'] = best_score = top_cv_score\n",
        "        model['transformers'] = []\n",
        "        all_scores = all_scores.append({'Model': model['name'], 'Function':'base_score','CV Score': '{:.2f} +/- {:.02}'.format(np.mean(cv_scores[cv_scores > 0.0]),np.std(cv_scores[cv_scores > 0.0])),'Holdout Score': top_cv_score, 'Difference': 0, 'Outcome': 'Base ' + model['name']}, ignore_index=True)\n",
        "\n",
        "        for transformer in transformers:\n",
        "            engineered_data = df.copy()\n",
        "            outcome = 'Rejected'\n",
        "\n",
        "            try:\n",
        "                top_transformer_score, transformer_scores, cv_model = cv_evaluate(engineered_data, model = model['model'], transformers = [transformer['transformer']], splits = splits)\n",
        "                difference = (top_transformer_score - best_score)\n",
        "\n",
        "                if difference > 0:\n",
        "                    model['transformers'] = [i for i in model['transformers'] if i['name'] != transformer['name']]\n",
        "                    model['transformers'].append(transformer['transformer'])\n",
        "                    outcome = 'Accepted'\n",
        "\n",
        "                mean = np.mean(transformer_scores[transformer_scores > 0.0])\n",
        "                std = np.std(transformer_scores[transformer_scores > 0.0])\n",
        "                if np.isnan(mean) or np.isnan(std):\n",
        "                    mean = 0.00\n",
        "                    std = 0.00\n",
        "\n",
        "                score = {'Model': model['name'], 'Function':transformer['name'],'CV Score': '{:.2f} +/- {:.02}'.format(mean,std),'Holdout Score': top_transformer_score, 'Difference': difference, 'Outcome': outcome}\n",
        "\n",
        "            except:\n",
        "                score = {'Model': model['name'], 'Function':transformer['name'],'CV Score': '0.00 +/- 0.00','Holdout Score': 0, 'Difference': 0, 'Outcome': 'Error'}\n",
        "\n",
        "            all_scores = all_scores.append(score, ignore_index=True)\n",
        "    return create_pipelines(models), all_scores\n",
        "\n",
        "def create_pipelines(pipes):\n",
        "    cachedir = mkdtemp()\n",
        "    for item in pipes:\n",
        "        item['pipeline'] = make_pipeline(*item['transformers'], item['model'], memory = cachedir)\n",
        "\n",
        "    return sorted(pipes, key=lambda k: k['score'], reverse = True)\n",
        "\n",
        "\n",
        "# Uso\n",
        "correlated_cols, categoric_cols, numeric_cols = correlated(df_tratada, 0.9)"
      ],
      "metadata": {
        "id": "lF_mGL9WJzxS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}